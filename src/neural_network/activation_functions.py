import numpy as np
from scipy.special import expit

"""
Este m√≥dulo foi gerado inicialmente pelo ChatGPT e posteriormente modificado manualmente 
para aprimoramento, inclus√£o de mais fun√ß√µes e ajustes espec√≠ficos.

M√≥dulo com diversas fun√ß√µes de ativa√ß√£o usadas em redes neurais.

Inclui fun√ß√µes populares (ReLU, Sigmoid, Tanh, Softmax) e algumas menos comuns (Swish, ELU, Leaky ReLU, GELU).

Cada fun√ß√£o possui:
- Overview: Explica√ß√£o breve do que a fun√ß√£o faz.
- Casos bons para uso: Quando a fun√ß√£o √© recomendada.
- Quando evitar: Situa√ß√µes onde n√£o √© a melhor escolha.

Autor: Renato Calabro
"""

def step_function(x):
    """
    Overview: Fun√ß√£o degrau bin√°ria, retorna 1 para x >= 0 e 0 para x < 0.
    Casos bons para uso: Perceptrons e redes neurais simples para classifica√ß√£o bin√°ria.
    Quando evitar: Em redes profundas, pois n√£o √© diferenci√°vel e n√£o permite aprendizado eficiente.
    """
    return np.where(x > 0, 1, 0)

def step_function_derivative(x):
    """
    Overview: Derivada da fun√ß√£o degrau, que √© zero em todos os pontos (n√£o √∫til para aprendizado).
    Casos bons para uso: Apenas para estudo te√≥rico, pois n√£o √© √∫til no treinamento de redes neurais.
    Quando evitar: Sempre que precisar de gradientes, pois a derivada √© sempre zero.
    """
    return np.zeros_like(x)

def sigmoid(x):
    """
    Overview: Fun√ß√£o de ativa√ß√£o sigmoide, que mapeia qualquer valor real para o intervalo (0,1).
    Casos bons para uso: Classifica√ß√£o bin√°ria e redes neurais simples.
    Quando evitar: Em redes profundas, pois sofre com o problema do gradiente desaparecendo.
    """
    return expit(x)  # Evita overflow num√©rico com scipy.special.expit

def sigmoid_derivative(x):
    """Derivada da fun√ß√£o Sigmoid."""
    return sigmoid(x) * (1 - sigmoid(x))

def relu(x):
    """
    Overview: ReLU (Rectified Linear Unit) retorna 0 para valores negativos e x para positivos.
    Casos bons para uso: Redes profundas devido √† efici√™ncia computacional e redu√ß√£o do gradiente desaparecendo.
    Quando evitar: Pode sofrer com o problema de neur√¥nios mortos (valores negativos sempre retornam 0).
    """
    return np.maximum(0, x)

def relu_derivative(x):
    """Derivada da fun√ß√£o ReLU."""
    return np.where(x > 0, 1, 0)

def leaky_relu(x, alpha=0.01):
    """
    Overview: Variante da ReLU que permite pequenos valores negativos em vez de 0.
    Casos bons para uso: Quando h√° risco de neur√¥nios mortos em redes profundas.
    Quando evitar: Se os valores negativos n√£o s√£o um problema, a ReLU padr√£o pode ser suficiente.
    """
    return np.where(x > 0, x, alpha * x)

def leaky_relu_derivative(x, alpha=0.01):
    """Derivada da fun√ß√£o Leaky ReLU."""
    return np.where(x > 0, 1, alpha)

def tanh(x):
    """
    Overview: Mapeia valores reais para o intervalo (-1,1), sendo semelhante ao Sigmoid, mas centrado em zero.
    Casos bons para uso: Quando os dados s√£o distribu√≠dos em torno de 0.
    Quando evitar: Pode sofrer com o gradiente desaparecendo em redes profundas.
    """
    return np.tanh(x)

def tanh_derivative(x):
    """Derivada da fun√ß√£o Tangente Hiperb√≥lica."""
    return 1 - np.tanh(x)**2

def softmax(x):
    """
    Overview: Converte um vetor de valores reais em probabilidades que somam 1.
    Casos bons para uso: Classifica√ß√£o multiclasse na camada de sa√≠da.
    Quando evitar: Se os valores de entrada forem muito grandes, pode haver problemas de estabilidade num√©rica.
    """
    exps = np.exp(x - np.max(x))  # Subtrai o m√°ximo para evitar overflow
    return exps / np.sum(exps, axis=-1, keepdims=True)

def softmax_derivative(x):
    """Derivada da fun√ß√£o Softmax (matriz Jacobiana)."""
    s = softmax(x)
    return np.diagflat(s) - np.outer(s, s)

def swish(x):
    """
    Overview: Ativa√ß√£o n√£o linear onde f(x) = x * sigmoid(x), permitindo valores negativos atenuados.
    Casos bons para uso: Redes profundas e modelos avan√ßados como EfficientNet.
    Quando evitar: Pode ser computacionalmente mais caro do que ReLU.
    """
    return x * sigmoid(x)

def swish_derivative(x):
    """Derivada da fun√ß√£o Swish."""
    return sigmoid(x) + x * sigmoid_derivative(x)

def elu(x, alpha=1.0):
    """
    Overview: Exponential Linear Unit (ELU) resolve o problema de neur√¥nios mortos da ReLU.
    Casos bons para uso: Redes profundas quando h√° muitos valores negativos.
    Quando evitar: √â mais caro computacionalmente do que ReLU ou Leaky ReLU.
    """
    return np.where(x > 0, x, alpha * (np.exp(x) - 1))

def elu_derivative(x, alpha=1.0):
    """Derivada da fun√ß√£o ELU."""
    return np.where(x > 0, 1, elu(x, alpha) + alpha)

def gelu(x):
    """
    Overview: Gaussian Error Linear Unit (GELU), ativa√ß√£o suave baseada na distribui√ß√£o Gaussiana.
    Casos bons para uso: Modelos de NLP, como BERT, devido √† suavidade da ativa√ß√£o.
    Quando evitar: Em redes pequenas, onde ReLU pode ser mais eficiente.
    """
    return 0.5 * x * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * x**3)))

def gelu_derivative(x):
    """Derivada da fun√ß√£o GELU."""
    return 0.5 * (1 + np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * x**3))) + \
        0.5 * x * (1 - np.tanh(np.sqrt(2 / np.pi) * (x + 0.044715 * x**3))**2) * \
        (np.sqrt(2 / np.pi) * (1 + 3 * 0.044715 * x**2))

# üîπ Dicion√°rio para importar fun√ß√µes de forma din√¢mica
ACTIVATIONS = {
    "step": step_function,
    "sigmoid": sigmoid,
    "relu": relu,
    "leaky_relu": leaky_relu,
    "tanh": tanh,
    "softmax": softmax,
    "swish": swish,
    "elu": elu,
    "gelu": gelu
}

# üîπ Dicion√°rio das derivadas das
# fun√ß√µes de ativa√ß√£o (para uso futuro no treinamento)
ACTIVATION_DERIVATIVES = {
    "step": step_function_derivative,
    "sigmoid": sigmoid_derivative,
    "relu": relu_derivative,
    "leaky_relu": leaky_relu_derivative,
    "tanh": tanh_derivative,
    "softmax": softmax_derivative,
    "swish": swish_derivative,
    "elu": elu_derivative,
    "gelu": gelu_derivative
}
