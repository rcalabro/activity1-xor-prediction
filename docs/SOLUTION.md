# ğŸ§  Atividade 1: Rede Neural para ClassificaÃ§Ã£o da Porta XOR

**Disciplina:** InteligÃªncia Artificial  
**Professor:** David Berto Farias  
**Aluno:** Renato Calabro  
**Entrega:** Projeto completo em Python puro, sem bibliotecas de machine learning.

---

## ğŸ¯ Objetivo

Construir uma **rede neural feedforward** em Python capaz de aprender e predizer o comportamento da porta lÃ³gica **XOR**, utilizando apenas NumPy e implementaÃ§Ãµes prÃ³prias de feedforward, funÃ§Ãµes de ativaÃ§Ã£o, backpropagation e cÃ¡lculo de erro.

---

## âš™ï¸ Arquitetura da Rede

Para modelar a porta XOR, utilizei uma rede neural com a seguinte estrutura:

    [2 entradas] â†’ [2 neurÃ´nios na camada oculta] â†’ [1 neurÃ´nio na saÃ­da]

- **Input layer:** 2 neurÃ´nios (representando as entradas A e B da porta XOR)
- **Hidden layer:** 2 neurÃ´nios
- **Output layer:** 1 neurÃ´nio

### ğŸ” Por que `2-2-1` (camadas)?

A funÃ§Ã£o XOR Ã© um clÃ¡ssico exemplo de **problema nÃ£o linearmente separÃ¡vel**. Isso significa que **nÃ£o existe uma linha reta (ou plano, ou hiperplano)** que consiga separar os casos onde a saÃ­da Ã© `1` dos casos onde a saÃ­da Ã© `0`.

Uma rede neural com apenas **camadas lineares (sem nÃ£o-linearidade)** nÃ£o consegue resolver esse tipo de problema.

Ao incluir uma camada com dois neurÃ´nios e usar uma funÃ§Ã£o de ativaÃ§Ã£o nÃ£o linear (como a sigmoide), a rede passa a conseguir â€œdobrarâ€ o espaÃ§o de decisÃ£o. Isso permite que ela separe corretamente os casos da XOR, mesmo que eles nÃ£o possam ser separados com uma linha reta.

> Essa arquitetura `2-2-1` Ã© a forma mais simples e eficiente de resolver a XOR com redes neurais.

## ğŸ§ª FunÃ§Ã£o de AtivaÃ§Ã£o: `sigmoid`

Utilizei a funÃ§Ã£o de ativaÃ§Ã£o **sigmoide** (`sigmoid(x) = 1 / (1 + e^-x)`) em todos os neurÃ´nios.

### âœ… Por que `sigmoid`?

- Retorna valores contÃ­nuos entre 0 e 1 â†’ ideal para classificaÃ§Ãµes binÃ¡rias
- Tem derivada simples â†’ facilita a implementaÃ§Ã£o do backpropagation
- Ajuda a suavizar os ajustes durante o treinamento

### âš ï¸ Quando **nÃ£o** usar sigmoid?

- Para redes muito profundas â†’ pode causar o problema do **gradiente desaparecendo**
- Quando a saÃ­da desejada for valores nÃ£o normalizados (ex: regressÃ£o)

## ğŸ“‰ FunÃ§Ã£o de Custo: `Binary Cross-Entropy`

Para o treinamento, utilizei a funÃ§Ã£o de custo **Binary Cross-Entropy**, apropriada para problemas de classificaÃ§Ã£o binÃ¡ria.

```python
loss(y, y_pred) = -[y * log(y_pred) + (1 - y) * log(1 - y_pred)]
```

### âœ… Por que Binary Cross-Entropy?

- Ã‰ **especializada para classificaÃ§Ãµes binÃ¡rias**
- Penaliza mais fortemente as prediÃ§Ãµes incorretas com alta confianÃ§a
- Gera gradientes mais Ãºteis para sigmoid (melhor que MSE nesse caso)

## ğŸ‹ï¸ ParÃ¢metros de Treinamento

A rede foi treinada com backpropagation "vanilla", implementado do zero, utilizando os seguintes hiperparÃ¢metros:

- **Ã‰pocas:** `1000`
- **Taxa de aprendizado (learning rate):** `1`
- **Erro alvo (`target_error`):** `0.25`

Esses valores foram **suficientes para resolver a porta XOR com sucesso**, atingindo o erro mÃ­nimo desejado de forma rÃ¡pida e estÃ¡vel.

## âœ… Resultado

A rede aprendeu corretamente a lÃ³gica da porta XOR, classificando corretamente os seguintes casos:

| Entrada A | Entrada B | SaÃ­da Esperada | SaÃ­da da Rede |
|-----------|-----------|----------------|----------------|
|     0     |     0     |       0        |       0        |
|     0     |     1     |       1        |       1        |
|     1     |     0     |       1        |       1        |
|     1     |     1     |       0        |       0        |

## ğŸ§® FunÃ§Ã£o de ClassificaÃ§Ã£o: `xor_classification`

ApÃ³s o treinamento da rede com saÃ­da contÃ­nua entre `0` e `1`, foi implementada uma funÃ§Ã£o de **pÃ³s-processamento** para transformar a saÃ­da da rede em **valores binÃ¡rios discretos** (`0` ou `1`), conforme o esperado pela porta XOR.

```python
def xor_classification(pred):
    def toClass(value):
        return (value > 0.5).astype(int)

    return np.array([toClass(x[0]) for x in pred])
```

### âœ… O que ela faz:

- Recebe a saÃ­da contÃ­nua da rede (ex: `[[0.82], [0.12]]`)
- Aplica um limiar de decisÃ£o `> 0.5`
- Converte cada saÃ­da em `1` ou `0` (`int`)

### âœ… Por que usar:

- A rede usa `sigmoid`, que retorna **valores contÃ­nuos**
- Para aplicaÃ§Ãµes com lÃ³gica binÃ¡ria como a porta XOR, precisamos **decidir** se o valor representa um `1` ou `0`
- Com isso, garantimos uma saÃ­da compatÃ­vel com a tabela verdade da porta XOR

A funÃ§Ã£o foi atribuÃ­da Ã  rede via o parÃ¢metro `output_classification`, o que permite que a rede **retorne diretamente o valor classificado** apÃ³s o `predict`, sem necessidade de processamento externo.

## ğŸ§° ObservaÃ§Ãµes

- Todo o projeto foi feito com **NumPy puro**, sem uso de frameworks de machine learning
- A estrutura foi feita para ser modular, com suporte a mÃºltiplas funÃ§Ãµes de ativaÃ§Ã£o e estratÃ©gias de treino
- O cÃ³digo pode ser expandido facilmente para resolver outras portas lÃ³gicas ou problemas nÃ£o lineares

## ğŸ”— HistÃ³rico de Desenvolvimento

Todo o histÃ³rico de desenvolvimento, incluindo decisÃµes de projeto, refatoraÃ§Ãµes, testes e melhorias contÃ­nuas, estÃ¡ disponÃ­vel publicamente no repositÃ³rio abaixo:

> ğŸ“‚ **GitHub:** [github.com/rcalabro/activity1-xor-prediction](https://github.com/rcalabro/activity1-xor-prediction)

Esse repositÃ³rio contÃ©m:
- Commits detalhados com cada etapa do progresso
- CÃ³digo modular e documentado
- Estrutura pensada para reusabilidade e expansÃ£o didÃ¡tica
